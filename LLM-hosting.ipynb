{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c360d4dd-0e8f-4afc-be08-f4250ad5f5bf",
   "metadata": {},
   "source": [
    "In this notebook I will walk you through how you can deploy a tuned LLM to sagemaker realtime endpoint.\n",
    "The LLm used here is a tuned Llama2-7b model.\n",
    "\n",
    "A few things sagemaker offers:\n",
    "* A variety of [hosting options](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html)\n",
    "* Conatiner [logs](https://docs.aws.amazon.com/sagemaker/latest/dg/logging-cloudwatch.html) and [metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html) all available in cloudwatch\n",
    "* Automatic metadata capture for model lineage\n",
    "* Robust selection of managed hosting images from popular [frameworks](https://sagemaker.readthedocs.io/en/stable/frameworks/index.html)\n",
    "\n",
    "In this example we would use a [Deep java library Serving](https://sagemaker.readthedocs.io/en/stable/frameworks/djl/using_djl.html) framework in hosting a tuned Llama7b model.\n",
    "\n",
    "To host on Sagemaker, model files must be in s3. For language models we expect that the model weights, model config, and tokenizer config are provided in S3.\n",
    "For example:\n",
    "```\n",
    "my_bucket/my_model/\n",
    "|- config.json\n",
    "|- added_tokens.json\n",
    "|- config.json\n",
    "|- pytorch_model-*-of-*.bin # model weights can be partitioned into multiple checkpoints\n",
    "|- tokenizer.json\n",
    "|- tokenizer_config.json\n",
    "|- vocab.json\n",
    "```\n",
    "\n",
    "The sagemaker managed DJL images come with a default inference image for serving, so you do not need to provide one. However, if you decide to provide one, you can pass it as a local path or an s3 uri (when using s3 uri, inference artifacts -sourcedir- must be compressed in a `tar.gz` format)\n",
    "Here is an example of a dir containing my inference artifacts:\n",
    "```\n",
    "sourcedir/\n",
    "|- script.py # Inference handler code\n",
    "|- serving.properties # Model Server configuration file\n",
    "|- requirements.txt # Additional Python requirements that will be installed at runtime via PyPi\n",
    "|- lib\n",
    "    |- *.whl files # In contratst to a requirements.txt, package wheel files to be installed at runtime \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459b6c16-b8ca-40bb-ace7-61c50f436970",
   "metadata": {},
   "source": [
    "IMPORT MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c17b487-25f2-47ce-8df6-15195803a888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b185f2-b148-4f04-9d33-5c68702d2ff7",
   "metadata": {},
   "source": [
    "The inference script I used to serve this model is the same used to host the LLama2 Models on Jumpstart. You can write your inference script to meet your needs.\n",
    "All SageMaker JumpStart artifacts are hosted in an s3 bucket managed by the service team.\n",
    "The inference artifacts for LLama2 models on JumpStart can be found here:\n",
    "* s3://jumpstart-cache-prod-{region}/source-directory-tarballs/meta/inference/textgeneration/v1.1.0/sourcedir.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6341266c-93f0-4907-a336-82cf202078e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mitertools\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36menum\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Enum\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtyping\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Any\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtyping\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dict\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtyping\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m List\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtyping\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Optional\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mdeepspeed\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdjl_python\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Input\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdjl_python\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Output\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdjl_python\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdeepspeed\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DeepSpeedService\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_jumpstart_huggingface_script_utilities\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdjl_python\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36minference\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtextgeneration\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m format_djl_output\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_jumpstart_huggingface_script_utilities\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdjl_python\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36minference\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtextgeneration\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m process_input\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_jumpstart_huggingface_script_utilities\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpayload\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdialog\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m convert_dialog_to_input_prompt\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_jumpstart_huggingface_script_utilities\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpayload\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36menums\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m GenerationConfigParams\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_jumpstart_huggingface_script_utilities\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpayload\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mstopping_criteria\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\n",
      "    add_stopping_criteria_to_model_kwargs,\n",
      ")\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LlamaForCausalLM\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LlamaTokenizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m pipeline\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpipelines\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtext_generation\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TextGenerationPipeline\n",
      "\n",
      "\n",
      "MODEL_DIR_STR = \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "MODEL_ID_STR = \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "TENSOR_PARALLEL_DEGREE_STR = \u001b[33m\"\u001b[39;49;00m\u001b[33mtensor_parallel_degree\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "MAX_TOKENS_STR = \u001b[33m\"\u001b[39;49;00m\u001b[33mmax_tokens\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "MODEL_TYPE_STR = \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "LOCAL_RANK_ENV_VAR_STR = \u001b[33m\"\u001b[39;49;00m\u001b[33mLOCAL_RANK\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "TP_SIZE_STR = \u001b[33m\"\u001b[39;49;00m\u001b[33mtp_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "AUTO_STR = \u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "POST_DS_INFERENCE_INIT_STR = \u001b[33m\"\u001b[39;49;00m\u001b[33mpost-ds-inference-init\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "TEXT_GENERATION_STR = \u001b[33m\"\u001b[39;49;00m\u001b[33mtext-generation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "GENERATED_TEXT_STR = \u001b[33m\"\u001b[39;49;00m\u001b[33mgenerated_text\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "GENERATION_STR = \u001b[33m\"\u001b[39;49;00m\u001b[33mgeneration\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "ACCEPT_EULA_STR = \u001b[33m\"\u001b[39;49;00m\u001b[33maccept_eula\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "ACCEPT_EULA_ERROR_MESSAGE_STR = (\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mNeed to pass custom_attributes=\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33maccept_eula=true\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m as part of header. This means you have read and accept the \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mend-user license agreement (EULA) of the model. EULA can be found in model card description or from \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mhttps://ai.meta.com/resources/models-and-libraries/llama-downloads/.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      ")\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mModelType\u001b[39;49;00m(\u001b[36mstr\u001b[39;49;00m, Enum):\n",
      "    \u001b[33m\"\"\"Fine-tuning category for model weights.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "    BASE = \u001b[33m\"\u001b[39;49;00m\u001b[33mbase\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    CHAT = \u001b[33m\"\u001b[39;49;00m\u001b[33mchat\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mLlama2Service\u001b[39;49;00m(DeepSpeedService):\n",
      "    \u001b[33m\"\"\"A service object for Llama 2 model family using the DJL Python engine.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m) -> \u001b[34mNone\u001b[39;49;00m:\n",
      "        \u001b[33m\"\"\"Set initialization flag to False, model to be initialized upon invocation of initialize method.\"\"\"\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.initialized = \u001b[34mFalse\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.pipeline: Optional[TextGenerationPipeline] = \u001b[34mNone\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.model_type: Optional[ModelType] = \u001b[34mNone\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32minitialize\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, properties: Dict[\u001b[36mstr\u001b[39;49;00m, Any]) -> \u001b[34mNone\u001b[39;49;00m:\n",
      "        \u001b[33m\"\"\"Load the model and initialize a transformers pipeline.\"\"\"\u001b[39;49;00m\n",
      "        local_rank = \u001b[36mint\u001b[39;49;00m(os.getenv(LOCAL_RANK_ENV_VAR_STR, \u001b[33m\"\u001b[39;49;00m\u001b[33m0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "        model_location = properties.get(MODEL_DIR_STR)\n",
      "        \u001b[34mif\u001b[39;49;00m MODEL_ID_STR \u001b[35min\u001b[39;49;00m properties:\n",
      "            model_location = properties.get(MODEL_ID_STR)\n",
      "        tensor_parallel = properties.get(TENSOR_PARALLEL_DEGREE_STR)\n",
      "        max_tokens = properties.get(MAX_TOKENS_STR)\n",
      "        \u001b[36mself\u001b[39;49;00m.model_type = ModelType(properties.get(MODEL_TYPE_STR))\n",
      "\n",
      "        tokenizer = LlamaTokenizer.from_pretrained(model_location)\n",
      "\n",
      "        model = LlamaForCausalLM.from_pretrained(\n",
      "            model_location,\n",
      "            torch_dtype=torch.bfloat16,\n",
      "            low_cpu_mem_usage=\u001b[34mTrue\u001b[39;49;00m,\n",
      "        )\n",
      "        model = model.eval()\n",
      "        torch.cuda.empty_cache()\n",
      "        model = deepspeed.init_inference(\n",
      "            model,\n",
      "            tensor_parallel={TP_SIZE_STR: tensor_parallel},\n",
      "            max_tokens=max_tokens,\n",
      "            dtype=model.dtype,\n",
      "            replace_method=AUTO_STR,\n",
      "            replace_with_kernel_inject=\u001b[34mTrue\u001b[39;49;00m,\n",
      "        )\n",
      "        torch.cuda.empty_cache()\n",
      "        deepspeed.runtime.utils.see_memory_usage(POST_DS_INFERENCE_INIT_STR, force=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[36mself\u001b[39;49;00m.pipeline = pipeline(TEXT_GENERATION_STR, model=model.module, tokenizer=tokenizer, device=local_rank)\n",
      "\n",
      "        \u001b[36mself\u001b[39;49;00m.initialized = \u001b[34mTrue\u001b[39;49;00m\n",
      "\n",
      "    \u001b[90m@format_djl_output\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32minference\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, inputs: Input) -> Output:\n",
      "        \u001b[33m\"\"\"Define customized inference method to have hyperparameter validation for text generation task.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m        Args:\u001b[39;49;00m\n",
      "\u001b[33m            inputs (djl_python.inputs.Input): input containing payload and content type.\u001b[39;49;00m\n",
      "\u001b[33m        Returns:\u001b[39;49;00m\n",
      "\u001b[33m            outputs (djl_python.inputs.Output): model prediction output.\u001b[39;49;00m\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\n",
      "        input_data, model_kwargs = process_input(inputs, input_data_as_list=\u001b[34mFalse\u001b[39;49;00m, use_parameters_key=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        model_kwargs = add_stopping_criteria_to_model_kwargs(model_kwargs, \u001b[36mself\u001b[39;49;00m.pipeline.tokenizer)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m GenerationConfigParams.RETURN_FULL_TEXT \u001b[35mnot\u001b[39;49;00m \u001b[35min\u001b[39;49;00m model_kwargs:\n",
      "            model_kwargs[GenerationConfigParams.RETURN_FULL_TEXT] = \u001b[34mFalse\u001b[39;49;00m\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m GenerationConfigParams.DO_SAMPLE \u001b[35mnot\u001b[39;49;00m \u001b[35min\u001b[39;49;00m model_kwargs:\n",
      "            model_kwargs[GenerationConfigParams.DO_SAMPLE] = \u001b[34mTrue\u001b[39;49;00m\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.model_type == ModelType.CHAT:\n",
      "            input_data = convert_dialog_to_input_prompt(input_data, \u001b[36mself\u001b[39;49;00m.pipeline.tokenizer)\n",
      "\n",
      "        model_output: List[List[Dict[\u001b[36mstr\u001b[39;49;00m, Any]]] = \u001b[36mself\u001b[39;49;00m.pipeline(input_data, **model_kwargs)\n",
      "        model_output = \u001b[36mlist\u001b[39;49;00m(itertools.chain(*model_output))\n",
      "        \u001b[34mfor\u001b[39;49;00m sequence \u001b[35min\u001b[39;49;00m model_output:\n",
      "            \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.model_type == ModelType.CHAT:\n",
      "                sequence[GENERATION_STR] = {\u001b[33m\"\u001b[39;49;00m\u001b[33mrole\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33massistant\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcontent\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: sequence.pop(GENERATED_TEXT_STR)}\n",
      "            \u001b[34melse\u001b[39;49;00m:\n",
      "                sequence[GENERATION_STR] = sequence.pop(GENERATED_TEXT_STR)\n",
      "        \u001b[34mreturn\u001b[39;49;00m model_output\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mhandle\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, inputs: Input) -> Optional[Output]:\n",
      "        \u001b[33m\"\"\"Handle an input query.\"\"\"\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.initialized \u001b[35mis\u001b[39;49;00m \u001b[34mFalse\u001b[39;49;00m:\n",
      "            \u001b[36mself\u001b[39;49;00m.initialize(inputs.get_properties())\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m inputs.is_empty():\n",
      "            \u001b[37m# Model server makes an empty call to warmup the model on startup\u001b[39;49;00m\n",
      "            \u001b[34mreturn\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\n",
      "\n",
      "        properties = inputs.get_properties()\n",
      "        eula = properties.get(ACCEPT_EULA_STR, \u001b[33m\"\u001b[39;49;00m\u001b[33mfalse\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).lower()\n",
      "        \u001b[34mif\u001b[39;49;00m eula != \u001b[33m\"\u001b[39;49;00m\u001b[33mtrue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "            \u001b[34mreturn\u001b[39;49;00m Output().error(ACCEPT_EULA_ERROR_MESSAGE_STR)\n",
      "\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.inference(inputs)\n",
      "\n",
      "\n",
      "_service = Llama2Service()\n",
      "handle = _service.handle\n"
     ]
    }
   ],
   "source": [
    "!pygmentize code/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1d7961-bc6d-401b-9f20-0512f8620ea9",
   "metadata": {},
   "source": [
    "We would be using the [DJL Deepspeed](https://sagemaker.readthedocs.io/en/stable/frameworks/djl/sagemaker.djl_inference.html#deepspeedmodel) image to host our LLama2 model. It comes prepackaged with certain [modules](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-dlc.html). For a full list of supported DL frameworks see  [deep-learning-containers](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2694c9d7-9730-49d4-875b-159bae76d546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.djl_inference.model import DeepSpeedModel\n",
    "djl_model = DeepSpeedModel(\n",
    "    \"s3://<path to model files>\", # S3 uri containing model files. This can also be a HuggingFace Hub model id\n",
    "    role, # sagemaker role  \n",
    "   source_dir=\"code\", # local dir holding your custom inference script and other dependencies\n",
    "    entry_point=\"inference.py\", # inference script located within the source_dir path\n",
    "     dtype=\"fp16\", # The data type to use for loading your model.\n",
    "     task=\"text-generation\",\n",
    "    model_loading_timeout=3600, \n",
    "    tensor_parallel_degree=1, # number of gpus to partition the model across using tensor parallelism\n",
    "    max_tokens=4096  #The maximum number of tokens (input + output tokens) the DeepSpeed engine is configured for\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8749cb3b-05eb-401a-ab36-e28e8fcc141c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "predictor = djl_model.deploy(\"ml.g5.4xlarge\", # Instance type\n",
    "                             initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aee521bf-221f-4bef-889c-93e744c5a2ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt=\"What is the capital of Nigeria?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d02ecdcc-820d-490c-9926-08cf2d9ed11f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"inputs\":  prompt,\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 5,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.03,   \n",
    "  }\n",
    "}\n",
    "\n",
    "# send request to endpoint\n",
    "response = predictor.predict(payload,\n",
    "                             custom_attributes='accept_eula=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8cf59854-faf4-416c-9e29-f887ce1e3df7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generation': '\\nWhat is the capital of Nigeria?\\nThe capital of Nigeria is Abuja.\\nWhat is the capital of Nigeria? The capital of Nigeria is Abuja. Check out this story on USATODAY.com: http://usat.ly/1bY43ZI\\nAP Published 12:00 a.m. ET March 17, 2013 | Updated 12:00 a.m. ET March 17, 2013\\nAbuja, Nigeria(Photo: AP)\\nThe capital of Nigeria is Abuja.\\nRead or Share this story: http://usat.ly/1bY43ZI'}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a42ed7-07ac-4d2b-82ca-af6ca3f86904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
